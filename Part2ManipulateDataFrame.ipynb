{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparksession\n",
    "ss = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Manipulation DataFrame\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = ss.read.json(\"./data/data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示数据类型\n",
    "显示数据类型的方法，可以使用 `printSchema()` 以及 `dtypes` 属性。前者是返回树形结构，后者是得到一个键值形式列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- altitude: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- exact_location: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- indoor: long (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |-- sampling_rate: string (nullable = true)\n",
      " |-- sensor: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- pin: string (nullable = true)\n",
      " |    |-- sensor_type: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- manufacturer: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- sensordatavalues: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |    |    |-- value_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 显示 Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('location',\n",
       "  'struct<altitude:string,country:string,exact_location:bigint,id:bigint,indoor:bigint,latitude:string,longitude:string>'),\n",
       " ('sampling_rate', 'string'),\n",
       " ('sensor',\n",
       "  'struct<id:bigint,pin:string,sensor_type:struct<id:bigint,manufacturer:string,name:string>>'),\n",
       " ('sensordatavalues',\n",
       "  'array<struct<id:bigint,value:string,value_type:string>>'),\n",
       " ('timestamp', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示DataFrame 维度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35527, 6)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示前几行数据\n",
    "有多种方法显示前几行数据，`head()`以及特殊的方法 `first()`、`show()` 等。需要注意返回的结果类型不是完全相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=5756852209, location=Row(altitude='104.9', country='UA', exact_location=0, id=22256, indoor=1, latitude='50.51', longitude='30.798'), sampling_rate=None, sensor=Row(id=36214, pin='7', sensor_type=Row(id=9, manufacturer='various', name='DHT22')), sensordatavalues=[Row(id=12224991603, value='10.00', value_type='temperature'), Row(id=12224991604, value='50.70', value_type='humidity')], timestamp='2019-12-13 11:10:02'),\n",
       " Row(id=5756852208, location=Row(altitude='111.8', country='GB', exact_location=1, id=21003, indoor=0, latitude='53.87869338867', longitude='-1.45841360092'), sampling_rate=None, sensor=Row(id=34792, pin='11', sensor_type=Row(id=17, manufacturer='Bosch', name='BME280')), sensordatavalues=[Row(id=12224991602, value='96357.16', value_type='pressure'), Row(id=12224991605, value='1.93', value_type='temperature'), Row(id=12224991606, value='100.00', value_type='humidity'), Row(id=None, value='97702.38', value_type='pressure_at_sealevel')], timestamp='2019-12-13 11:10:02')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=5756852209, location=Row(altitude='104.9', country='UA', exact_location=0, id=22256, indoor=1, latitude='50.51', longitude='30.798'), sampling_rate=None, sensor=Row(id=36214, pin='7', sensor_type=Row(id=9, manufacturer='various', name='DHT22')), sensordatavalues=[Row(id=12224991603, value='10.00', value_type='temperature'), Row(id=12224991604, value='50.70', value_type='humidity')], timestamp='2019-12-13 11:10:02')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "|        id|            location|sampling_rate|              sensor|    sensordatavalues|          timestamp|\n",
      "+----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "|5756852209|[104.9, UA, 0, 22...|         null|[36214, 7, [9, va...|[[12224991603, 10...|2019-12-13 11:10:02|\n",
      "|5756852208|[111.8, GB, 1, 21...|         null|[34792, 11, [17, ...|[[12224991602, 96...|2019-12-13 11:10:02|\n",
      "+----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示数据列名称\n",
    "通过显示 `schema` 和 `dtypes` 可以显示列名称，需要直接得到列名称可以直接通过 `columns` 属性获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'location', 'sampling_rate', 'sensor', 'sensordatavalues', 'timestamp']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值统计\n",
    "DataFrame 没有自带的检测是否为缺失值方法，需要通过调用 `functions` 模块中的方法可以进行统计缺失值，大致步骤如下：\n",
    "1. 通过 `isnull()` 判断是否为缺失值，此外需要注意该方法和 pandas 有差异——pandas 没有区分 NAN 和 NULL\n",
    "2. 使用 `cast()` 方法将数据类型转换为 `integer`。该步骤可以通过 [`when()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.when)  和 [`otherwise()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.otherwise)方法完成\n",
    "3. 通过 `sum()` 方法累计求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 pandas 的 isnull 方法判断\n",
    "pd.isnull(np.nan), pd.isnull(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sampling_rate|\n",
      "+-------------+\n",
      "|        35527|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    functions.sum(\n",
    "        functions.isnull(\"sampling_rate\").cast(\"integer\")\n",
    "    ).alias(\"sampling_rate\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Miss_value|\n",
      "+----------+\n",
      "|     35527|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 第二种方法，通过 when 和 otherwise 完成——实际内核是 SQL 中 CASE WHEN 方法\n",
    "# 注意 when 和 otherwise 可以链式表达而不需要额外的\n",
    "df.select(\n",
    "    functions.when(\n",
    "        functions.isnull(\"sampling_rate\"), 1\n",
    "    ).otherwise(0).alias(\"sampling_rate\")\n",
    ").select(functions.sum(\"sampling_rate\").alias(\"Miss_value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+------+----------------+---------+\n",
      "| id|location|sampling_rate|sensor|sensordatavalues|timestamp|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "|  0|       0|        35527|     0|               0|        0|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 计算 columns 的缺失值数量\n",
    "df.select([functions.sum(functions.when(functions.isnull(c), 1).otherwise(0)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+------+----------------+---------+\n",
      "| id|location|sampling_rate|sensor|sensordatavalues|timestamp|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "|  0|       0|        35527|     0|               0|        0|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 替换方法计算 columns 缺失值\n",
    "df.select(\n",
    "        [functions.sum(\n",
    "            functions.isnull(x).cast(\"integer\")\n",
    "        ).alias(x)\n",
    "         for x in df.columns\n",
    "        ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过 UDF 计算，需要先分别检验各列中缺失值，在进行计算\n",
    "def check_missing(x):\n",
    "    if x is None:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "missing_count = functions.udf(lambda x: check_missing(x), types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sum(test)|\n",
      "+---------+\n",
      "|    35527|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 计算单一列\n",
    "df.select(missing_count(\"sampling_rate\").alias(\"test\")).select(functions.sum(\"test\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+------+----------------+---------+\n",
      "| id|location|sampling_rate|sensor|sensordatavalues|timestamp|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "|  0|       0|        35527|     0|               0|        0|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\n",
    "    missing_count(x).alias(x) for x in df.columns\n",
    "    ]).select([\n",
    "        functions.sum(x).alias(x) for x in df.columns\n",
    "    ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+------+----------------+---------+\n",
      "| id|location|sampling_rate|sensor|sensordatavalues|timestamp|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "|  0|       0|        35527|     0|               0|        0|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 * 可以快速筛选列，并且可以定制化通配符筛选\n",
    "df.select([\n",
    "    missing_count(x).alias(x) for x in df.columns\n",
    "    ]).select([\n",
    "        functions.sum(x).alias(x) for x in df.columns\n",
    "    ]).select(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示基本的统计信息\n",
    "pandas 中可以通过 `DataFrame.describe()` 查看各列的统计信息。Spark 也支持该功能，可以使用 `summary()` 和 `describe()` 方法查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+-------------------+\n",
      "|summary|                 id|sampling_rate|          timestamp|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "|  count|              35527|            0|              35527|\n",
      "|   mean|5.756834376153911E9|         null|               null|\n",
      "| stddev| 10295.038530963458|         null|               null|\n",
      "|    min|         5756816550|         null|2019-12-13 11:04:53|\n",
      "|    max|         5756852209|         null|2019-12-13 11:10:02|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+-------------------+\n",
      "|summary|                 id|sampling_rate|          timestamp|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "|  count|              35527|            0|              35527|\n",
      "|   mean|5.756834376153911E9|         null|               null|\n",
      "| stddev| 10295.038530963458|         null|               null|\n",
      "|    min|         5756816550|         null|2019-12-13 11:04:53|\n",
      "|    25%|         5756825456|         null|               null|\n",
      "|    50%|         5756834377|         null|               null|\n",
      "|    75%|         5756843280|         null|               null|\n",
      "|    max|         5756852209|         null|2019-12-13 11:10:02|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据冗余统计\n",
    "Spark 统计数据冗余，和 Pandas 存在差异——pandas 直接提供了 `DataFrame.duplicate()` 方法判断数据是否冗余。Spark 需要通过多步骤完成，[countDistinc](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.countDistinct) 统计唯一值数量——其中非数值数据也是统计为一个唯一值， [count](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.count) 统计数据长度。\n",
    "\n",
    "当需要以多字段判断是否有冗余值时，需要通过 groupby 计数数量大于 1 的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1='R', col2=0.5188218355178833, col3=13, col4=0.0),\n",
       " Row(col1='X', col2=4.821608543395996, col3=4, col4=3.0)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建数据\n",
    "schema = types.StructType([\n",
    "    types.StructField(\"col1\", types.StringType()),\n",
    "    types.StructField(\"col2\", types.FloatType()),\n",
    "    types.StructField(\"col3\", types.IntegerType()),\n",
    "    types.StructField(\"col4\", types.FloatType())\n",
    "])\n",
    "\n",
    "test = ss.createDataFrame(\n",
    "    list(zip(\n",
    "        np.random.choice(list(string.ascii_uppercase), size=26).tolist(), \n",
    "        np.random.normal(0, 10,  size=26).tolist(), \n",
    "        np.random.randint(0, 20,  size=26).tolist(),\n",
    "        np.random.choice(list(range(4)) + [np.nan], size=26).tolist())), schema=schema\n",
    ")\n",
    "\n",
    "test.persist()\n",
    "\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|  26|  26|  26|  26|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计各列的长度\n",
    "test.select([functions.count(x).alias(x) for x in test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col4|\n",
      "+----+\n",
      "| 2.0|\n",
      "| 3.0|\n",
      "| 1.0|\n",
      "| NaN|\n",
      "| 0.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 该方法将缺失值也是统计为了唯一值\n",
    "test.select(\"col4\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|  19|  26|  14|   5|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计各列唯一值数量\n",
    "test.select([functions.countDistinct(x).alias(x) for x in test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   7|   0|  12|  21|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过两个长度值减去唯一值长度计算数据冗余数量\n",
    "test.select([(functions.count(x) - functions.countDistinct(x)).alias(x) for x in test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------------+-------------+-----------------------+----------------+\n",
      "|count(id)|count(location)|count(sampling_rate)|count(sensor)|count(sensordatavalues)|count(timestamp)|\n",
      "+---------+---------------+--------------------+-------------+-----------------------+----------------+\n",
      "|    35527|          35527|                   0|        35527|                  35527|           35527|\n",
      "+---------+---------------+--------------------+-------------+-----------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().select([functions.count(x) for x in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|col1|col4|count|\n",
      "+----+----+-----+\n",
      "|   D| NaN|    2|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计个组合下的冗余数量\n",
    "test.groupBy(\"col1\", \"col4\").count().filter(\"count > 1\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类型转换\n",
    "\n",
    "* 日期或时间戳转换为日期字符串 使用 [date_format()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.date_format) 方法进行转换\n",
    "* 日期字符串转换转换为日期或时间戳 使用 [to_date()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.to_date) 和 [to_timestamp](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.to_timestamp) 方法进行转换，如果没有设置 `format` 参数前者相当于使用 `col.cast(\"date\")`后者相当于使用了 `col.cast(\"timestamp\")`。需要注意两者在使用的格式需要依据 [Java 模式格式](https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html)\n",
    "* 比较通用的转换方法是使用 [cast()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast) 进行类型转换——思路和 SQL 中 `cast()` 相似。此外还有类似的 [astype()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.astype)\n",
    "\n",
    "需要注意第三种方法是 `Column` 的方法，在使用的时候最好强制申明 `df.select(functions.col(\"datetime\").cast(\"string\"))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 'timestamp')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换为时间戳\n",
    "df.select(functions.to_timestamp(\"timestamp\", \"yyyy-MM-dd hh:mm:ss\").alias(\"time\")).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 'string')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 时间戳转换为字符串\n",
    "df.select(\n",
    "    functions.to_timestamp(\"timestamp\", \"yyyy-MM-dd hh:mm:ss\").alias(\"time\")\n",
    ").select(\n",
    "    functions.date_format(\"time\", \"yyyy-MM-dd hh:mm:ss\").alias(\"time\")\n",
    ").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('col2', 'string')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 cast 方法\n",
    "test.select(functions.col(\"col2\").cast(\"string\")).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date_=datetime.date(2019, 12, 13)),\n",
       " Row(date_=datetime.date(2019, 12, 13))]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 astype 方法\n",
    "df.select(functions.col(\"timestamp\").cast(\"date\").alias(\"date_\")).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据筛选\n",
    "* 非缺失值条件筛选——使用 [isNotNull](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.isNotNull)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
