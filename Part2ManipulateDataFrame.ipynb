{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparksession\n",
    "ss = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Manipulation DataFrame\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = ss.read.json(\"./data/data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示数据类型\n",
    "显示数据类型的方法，可以使用 `printSchema()` 以及 `dtypes` 属性。前者是返回树形结构，后者是得到一个键值形式列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- altitude: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- exact_location: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- indoor: long (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |-- sampling_rate: string (nullable = true)\n",
      " |-- sensor: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- pin: string (nullable = true)\n",
      " |    |-- sensor_type: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- manufacturer: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- sensordatavalues: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |    |    |-- value_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 显示 Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'bigint'),\n",
       " ('location',\n",
       "  'struct<altitude:string,country:string,exact_location:bigint,id:bigint,indoor:bigint,latitude:string,longitude:string>'),\n",
       " ('sampling_rate', 'string'),\n",
       " ('sensor',\n",
       "  'struct<id:bigint,pin:string,sensor_type:struct<id:bigint,manufacturer:string,name:string>>'),\n",
       " ('sensordatavalues',\n",
       "  'array<struct<id:bigint,value:string,value_type:string>>'),\n",
       " ('timestamp', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示DataFrame 维度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35527, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示前几行数据\n",
    "有多种方法显示前几行数据，`head()`以及特殊的方法 `first()`、`show()`、`limit()` 等。需要注意返回的结果类型不是完全相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=5756852209, location=Row(altitude='104.9', country='UA', exact_location=0, id=22256, indoor=1, latitude='50.51', longitude='30.798'), sampling_rate=None, sensor=Row(id=36214, pin='7', sensor_type=Row(id=9, manufacturer='various', name='DHT22')), sensordatavalues=[Row(id=12224991603, value='10.00', value_type='temperature'), Row(id=12224991604, value='50.70', value_type='humidity')], timestamp='2019-12-13 11:10:02'),\n",
       " Row(id=5756852208, location=Row(altitude='111.8', country='GB', exact_location=1, id=21003, indoor=0, latitude='53.87869338867', longitude='-1.45841360092'), sampling_rate=None, sensor=Row(id=34792, pin='11', sensor_type=Row(id=17, manufacturer='Bosch', name='BME280')), sensordatavalues=[Row(id=12224991602, value='96357.16', value_type='pressure'), Row(id=12224991605, value='1.93', value_type='temperature'), Row(id=12224991606, value='100.00', value_type='humidity'), Row(id=None, value='97702.38', value_type='pressure_at_sealevel')], timestamp='2019-12-13 11:10:02')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=5756852209, location=Row(altitude='104.9', country='UA', exact_location=0, id=22256, indoor=1, latitude='50.51', longitude='30.798'), sampling_rate=None, sensor=Row(id=36214, pin='7', sensor_type=Row(id=9, manufacturer='various', name='DHT22')), sensordatavalues=[Row(id=12224991603, value='10.00', value_type='temperature'), Row(id=12224991604, value='50.70', value_type='humidity')], timestamp='2019-12-13 11:10:02')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "|        id|            location|sampling_rate|              sensor|    sensordatavalues|          timestamp|\n",
      "+----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "|5756852209|[104.9, UA, 0, 22...|         null|[36214, 7, [9, va...|[[12224991603, 10...|2019-12-13 11:10:02|\n",
      "|5756852208|[111.8, GB, 1, 21...|         null|[34792, 11, [17, ...|[[12224991602, 96...|2019-12-13 11:10:02|\n",
      "+----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示数据列名称\n",
    "通过显示 `schema` 和 `dtypes` 可以显示列名称，需要直接得到列名称可以直接通过 `columns` 属性获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'location', 'sampling_rate', 'sensor', 'sensordatavalues', 'timestamp']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失值统计\n",
    "DataFrame 没有自带的检测是否为缺失值方法，需要通过调用 `functions` 模块中的方法可以进行统计缺失值，大致步骤如下：\n",
    "1. 通过 `isnull()` 判断是否为缺失值，此外需要注意该方法和 pandas 有差异——pandas 没有区分 NAN 和 NULL。Spark 支持 [eqNullSafe](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.eqNullSafe) 安全测试\n",
    "2. 使用 `cast()` 方法将数据类型转换为 `integer`。该步骤可以通过 [`when()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.when)  和 [`otherwise()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.otherwise)方法完成\n",
    "3. 通过 `sum()` 方法累计求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 pandas 的 isnull 方法判断\n",
    "pd.isnull(np.nan), pd.isnull(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sampling_rate|\n",
      "+-------------+\n",
      "|        35527|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    functions.sum(\n",
    "        functions.isnull(\"sampling_rate\").cast(\"integer\")\n",
    "    ).alias(\"sampling_rate\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Miss_value|\n",
      "+----------+\n",
      "|     35527|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 第二种方法，通过 when 和 otherwise 完成——实际内核是 SQL 中 CASE WHEN 方法\n",
    "# 注意 when 和 otherwise 可以链式表达而不需要额外的\n",
    "df.select(\n",
    "    functions.when(\n",
    "        functions.isnull(\"sampling_rate\"), 1\n",
    "    ).otherwise(0).alias(\"sampling_rate\")\n",
    ").select(functions.sum(\"sampling_rate\").alias(\"Miss_value\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+------+----------------+---------+\n",
      "| id|location|sampling_rate|sensor|sensordatavalues|timestamp|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "|  0|       0|        35527|     0|               0|        0|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 计算 columns 的缺失值数量\n",
    "df.select([functions.sum(functions.when(functions.isnull(c), 1).otherwise(0)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+------+----------------+---------+\n",
      "| id|location|sampling_rate|sensor|sensordatavalues|timestamp|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "|  0|       0|        35527|     0|               0|        0|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 替换方法计算 columns 缺失值\n",
    "df.select(\n",
    "        [functions.sum(\n",
    "            functions.isnull(x).cast(\"integer\")\n",
    "        ).alias(x)\n",
    "         for x in df.columns\n",
    "        ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过 UDF 计算，需要先分别检验各列中缺失值，在进行计算\n",
    "def check_missing(x):\n",
    "    if x is None:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "missing_count = functions.udf(lambda x: check_missing(x), types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sum(test)|\n",
      "+---------+\n",
      "|    35527|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 计算单一列\n",
    "df.select(missing_count(\"sampling_rate\").alias(\"test\")).select(functions.sum(\"test\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+------+----------------+---------+\n",
      "| id|location|sampling_rate|sensor|sensordatavalues|timestamp|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "|  0|       0|        35527|     0|               0|        0|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\n",
    "    missing_count(x).alias(x) for x in df.columns\n",
    "    ]).select([\n",
    "        functions.sum(x).alias(x) for x in df.columns\n",
    "    ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+------+----------------+---------+\n",
      "| id|location|sampling_rate|sensor|sensordatavalues|timestamp|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "|  0|       0|        35527|     0|               0|        0|\n",
      "+---+--------+-------------+------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 * 可以筛选列，并且可以定制化通配符筛选\n",
    "df.select([\n",
    "    missing_count(x).alias(x) for x in df.columns\n",
    "    ]).select([\n",
    "        functions.sum(x).alias(x) for x in df.columns\n",
    "    ]).select(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示基本的统计信息\n",
    "pandas 中可以通过 `DataFrame.describe()` 查看各列的统计信息。Spark 也支持该功能，可以使用 `summary()` 和 `describe()` 方法查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+-------------------+\n",
      "|summary|                 id|sampling_rate|          timestamp|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "|  count|              35527|            0|              35527|\n",
      "|   mean|5.756834376153911E9|         null|               null|\n",
      "| stddev| 10295.038530963458|         null|               null|\n",
      "|    min|         5756816550|         null|2019-12-13 11:04:53|\n",
      "|    max|         5756852209|         null|2019-12-13 11:10:02|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+-------------------+\n",
      "|summary|                 id|sampling_rate|          timestamp|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "|  count|              35527|            0|              35527|\n",
      "|   mean|5.756834376153911E9|         null|               null|\n",
      "| stddev| 10295.038530963458|         null|               null|\n",
      "|    min|         5756816550|         null|2019-12-13 11:04:53|\n",
      "|    25%|         5756825456|         null|               null|\n",
      "|    50%|         5756834377|         null|               null|\n",
      "|    75%|         5756843280|         null|               null|\n",
      "|    max|         5756852209|         null|2019-12-13 11:10:02|\n",
      "+-------+-------------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据冗余统计\n",
    "Spark 统计数据冗余，和 Pandas 存在差异——pandas 直接提供了 `DataFrame.duplicate()` 方法判断数据是否冗余。Spark 需要通过多步骤完成，[countDistinc](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.countDistinct) 统计唯一值数量——其中非数值数据也是统计为一个唯一值， [count](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.count) 统计数据长度。\n",
    "\n",
    "当需要以多字段判断是否有冗余值时，需要通过 groupby 计数数量大于 1 的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1='T', col2=-10.2372465133667, col3=18, col4=3.0, col5=None),\n",
       " Row(col1='O', col2=1.5313410758972168, col3=9, col4=0.0, col5=None)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建数据\n",
    "schema = types.StructType([\n",
    "    types.StructField(\"col1\", types.StringType()),\n",
    "    types.StructField(\"col2\", types.FloatType()),\n",
    "    types.StructField(\"col3\", types.IntegerType()),\n",
    "    types.StructField(\"col4\", types.FloatType()),\n",
    "     types.StructField(\"col5\", types.StringType())\n",
    "])\n",
    "\n",
    "test = ss.createDataFrame(\n",
    "    list(zip(\n",
    "        np.random.choice(list(string.ascii_uppercase)+[None]*20, size=30).tolist(), \n",
    "        np.random.normal(0, 10,  size=30).tolist(), \n",
    "        np.random.randint(0, 20,  size=30).tolist(),\n",
    "        np.random.choice(list(range(4)) + [np.nan], size=30).tolist(),\n",
    "        np.random.choice(list(string.ascii_uppercase)+[None]*20, size=30).tolist())), schema=schema\n",
    ")\n",
    "\n",
    "test.persist()\n",
    "\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|col1|col2|col3|col4|col5|\n",
      "+----+----+----+----+----+\n",
      "|  21|  30|  30|  30|  13|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计各列的长度\n",
    "test.select([functions.count(x).alias(x) for x in test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col4|\n",
      "+----+\n",
      "| 2.0|\n",
      "| 3.0|\n",
      "| 1.0|\n",
      "| NaN|\n",
      "| 0.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 该方法将缺失值也是统计为了唯一值\n",
    "test.select(\"col4\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|col1|col2|col3|col4|col5|\n",
      "+----+----+----+----+----+\n",
      "|  14|  30|  14|   5|  10|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计各列唯一值数量\n",
    "test.select([functions.countDistinct(x).alias(x) for x in test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+\n",
      "|col1|col2|col3|col4|col5|\n",
      "+----+----+----+----+----+\n",
      "|   7|   0|  16|  25|   3|\n",
      "+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过两个长度值减去唯一值长度计算数据冗余数量\n",
    "test.select([(functions.count(x) - functions.countDistinct(x)).alias(x) for x in test.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|col1|col4|count|\n",
      "+----+----+-----+\n",
      "|null| 3.0|    3|\n",
      "|   W| 0.0|    2|\n",
      "|null| NaN|    3|\n",
      "|   T| 3.0|    2|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计各组合下的冗余数量\n",
    "test.groupBy(\"col1\", \"col4\").count().filter(\"count > 1\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类型转换\n",
    "\n",
    "* 日期或时间戳转换为日期字符串 使用 [date_format()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.date_format) 方法进行转换\n",
    "* 日期字符串转换转换为日期或时间戳 使用 [to_date()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.to_date) 和 [to_timestamp](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.to_timestamp) 方法进行转换，如果没有设置 `format` 参数前者相当于使用 `col.cast(\"date\")`后者相当于使用了 `col.cast(\"timestamp\")`。需要注意两者在使用的格式需要依据 [Java 模式格式](https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html)\n",
    "* 比较通用的转换方法是使用 [cast()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast) 进行类型转换——思路和 SQL 中 `cast()` 相似。此外还有类似的 [astype()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.astype)\n",
    "\n",
    "需要注意第三种方法是 `Column` 的方法，在使用的时候最好强制使用 `df.select(functions.col(\"datetime\").cast(\"string\"))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 'timestamp')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换为时间戳\n",
    "df.select(functions.to_timestamp(\"timestamp\", \"yyyy-MM-dd hh:mm:ss\").alias(\"time\")).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 'string')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 时间戳转换为字符串\n",
    "df.select(\n",
    "    functions.to_timestamp(\"timestamp\", \"yyyy-MM-dd hh:mm:ss\").alias(\"time\")\n",
    ").select(\n",
    "    functions.date_format(\"time\", \"yyyy-MM-dd hh:mm:ss\").alias(\"time\")\n",
    ").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('col2', 'string')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 cast 方法\n",
    "test.select(functions.col(\"col2\").cast(\"string\")).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date_=datetime.date(2019, 12, 13)),\n",
       " Row(date_=datetime.date(2019, 12, 13))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 astype 方法\n",
    "df.select(functions.col(\"timestamp\").astype(\"date\").alias(\"date_\")).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据筛选\n",
    "在 Spark 中进行数据筛选，可以通过 [filter()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter) 或者 [where()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where) 方法配合条件进行筛选。两者是 `DataFrame` 下的方法。\n",
    "\n",
    "* 非缺失值条件筛选 单列缺失值判断可以直接使用 [isNotNull()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.isNotNull)，多列筛选需要使用  '&' 替换 'and', '|' 替换 'or', '~' 替换 'not' ——但是通过 SQL 表达式时，需要直接使用逻辑运算的原始自符\n",
    "* 缺失值条件筛选 单列缺失值判断直接使用 [isNull()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.isNull)，在 `filter()` 中可以使用 `<col_name> is NULL` 进行筛选，多列筛选需要使用  '&' 替换 'and', '|' 替换 'or', '~' 替换 'not' ——但是通过 SQL 表达式时，需要直接使用逻辑运算的原始自符\n",
    "* 其他条件筛选 pandas 中可以通过掩码方式筛选数据，Spark 中需要借助 `filter()` 和 `where()` 构造条件表达式筛选——例如 `df.filter(df.id > 12).show()` 或者 `df.filter(\"id > 12\").show()`\n",
    "\n",
    "从上面的结果可以看出，构造出可用条件是非常重要的。Spark 的 [Column](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column) 提供了丰富的方法，例如 `endswith()`和 `startswith()`——不支持正则表达式，`isin()`，`like()`——支持类似 SQL 中 like 方式，`rlike()`——支持正则表达式，`contains()`，`between()`—— ` df.select(df.age.between(2, 4)).show()` 类似于使用 `WHERE age BETWEEN 2 AND 4; `\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----+----+----+\n",
      "|col1|       col2|col3|col4|col5|\n",
      "+----+-----------+----+----+----+\n",
      "|   T|-10.2372465|  18| 3.0|null|\n",
      "|   O|  1.5313411|   9| 0.0|null|\n",
      "|   S| -9.1716175|   5| 0.0|   J|\n",
      "|   X|   5.094148|   3| NaN|   X|\n",
      "|   E|  -3.525113|   3| 2.0|null|\n",
      "|   W|  -18.93121|  10| 2.0|   R|\n",
      "|   T| -7.7867312|   2| 3.0|null|\n",
      "|   N| -14.736388|   6| 3.0|null|\n",
      "|   R| -3.1653242|   7| 0.0|null|\n",
      "|   S|  27.000383|   8| 1.0|null|\n",
      "|   Z|  12.456934|  19| NaN|   K|\n",
      "|   B| -3.7770996|  18| 2.0|null|\n",
      "|   H|   2.598025|   9| NaN|null|\n",
      "|   M|  16.468563|  17| 2.0|   I|\n",
      "|   W| -5.6359506|   7| 0.0|   A|\n",
      "|   B| -13.762591|  19| NaN|   S|\n",
      "|   F| -0.5584573|  14| 0.0|null|\n",
      "|   R|  5.4427624|   4| 1.0|   L|\n",
      "|   W| 0.61386025|  10| 0.0|null|\n",
      "|   X| -4.4530787|   9| 1.0|   I|\n",
      "+----+-----------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 where 进行筛选非缺失列\n",
    "test.where(test.col1.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----+----+----+\n",
      "|col1|       col2|col3|col4|col5|\n",
      "+----+-----------+----+----+----+\n",
      "|   T|-10.2372465|  18| 3.0|null|\n",
      "|   O|  1.5313411|   9| 0.0|null|\n",
      "|   S| -9.1716175|   5| 0.0|   J|\n",
      "|   X|   5.094148|   3| NaN|   X|\n",
      "|   E|  -3.525113|   3| 2.0|null|\n",
      "|   W|  -18.93121|  10| 2.0|   R|\n",
      "|   T| -7.7867312|   2| 3.0|null|\n",
      "|   N| -14.736388|   6| 3.0|null|\n",
      "|   R| -3.1653242|   7| 0.0|null|\n",
      "|   S|  27.000383|   8| 1.0|null|\n",
      "|   Z|  12.456934|  19| NaN|   K|\n",
      "|   B| -3.7770996|  18| 2.0|null|\n",
      "|   H|   2.598025|   9| NaN|null|\n",
      "|   M|  16.468563|  17| 2.0|   I|\n",
      "|   W| -5.6359506|   7| 0.0|   A|\n",
      "|   B| -13.762591|  19| NaN|   S|\n",
      "|   F| -0.5584573|  14| 0.0|null|\n",
      "|   R|  5.4427624|   4| 1.0|   L|\n",
      "|   W| 0.61386025|  10| 0.0|null|\n",
      "|   X| -4.4530787|   9| 1.0|   I|\n",
      "+----+-----------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 filter 筛选非缺失列\n",
    "test.filter(test.col1.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----+----+----+\n",
      "|col1|      col2|col3|col4|col5|\n",
      "+----+----------+----+----+----+\n",
      "|null|  7.406317|  19| 1.0|null|\n",
      "|null| -4.990888|   3| NaN|null|\n",
      "|null|   8.07055|  15| NaN|null|\n",
      "|null| 2.4262414|  19| 2.0|   K|\n",
      "|null|-4.2632003|   2| NaN|   V|\n",
      "|null|-3.3306575|  10| 0.0|   E|\n",
      "|null| -16.01117|   8| 3.0|null|\n",
      "|null|-1.0873488|   8| 3.0|null|\n",
      "|null| 3.2200346|  18| 3.0|   X|\n",
      "+----+----------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 条件筛选缺失行， 等价于\n",
    "# test.where(\"col1 is NULL\").show()\n",
    "# test.filter(test.col1.isNull()).show()\n",
    "# test.where(test.col1.isNull()).show()\n",
    "test.filter(\"col1 is NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----+----+----+\n",
      "|col1|      col2|col3|col4|col5|\n",
      "+----+----------+----+----+----+\n",
      "|null|  7.406317|  19| 1.0|null|\n",
      "|null| -4.990888|   3| NaN|null|\n",
      "|null|   8.07055|  15| NaN|null|\n",
      "|null| -16.01117|   8| 3.0|null|\n",
      "|null|-1.0873488|   8| 3.0|null|\n",
      "+----+----------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 多列筛选缺失行，等价于\n",
    "# test.filter(\"col1 is NULL and col5 is NULL\").show()\n",
    "test.filter(test.col1.isNull() & test.col5.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=5756852209, location=Row(altitude='104.9', country='UA', exact_location=0, id=22256, indoor=1, latitude='50.51', longitude='30.798'), sampling_rate=None, sensor=Row(id=36214, pin='7', sensor_type=Row(id=9, manufacturer='various', name='DHT22')), sensordatavalues=[Row(id=12224991603, value='10.00', value_type='temperature'), Row(id=12224991604, value='50.70', value_type='humidity')], timestamp='2019-12-13 11:10:02'),\n",
       " Row(id=5756852208, location=Row(altitude='111.8', country='GB', exact_location=1, id=21003, indoor=0, latitude='53.87869338867', longitude='-1.45841360092'), sampling_rate=None, sensor=Row(id=34792, pin='11', sensor_type=Row(id=17, manufacturer='Bosch', name='BME280')), sensordatavalues=[Row(id=12224991602, value='96357.16', value_type='pressure'), Row(id=12224991605, value='1.93', value_type='temperature'), Row(id=12224991606, value='100.00', value_type='humidity'), Row(id=None, value='97702.38', value_type='pressure_at_sealevel')], timestamp='2019-12-13 11:10:02')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "|        id|            location|sampling_rate|              sensor|    sensordatavalues|          timestamp|\n",
      "+----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "|5756852209|[104.9, UA, 0, 22...|         null|[36214, 7, [9, va...|[[12224991603, 10...|2019-12-13 11:10:02|\n",
      "|5756852208|[111.8, GB, 1, 21...|         null|[34792, 11, [17, ...|[[12224991602, 96...|2019-12-13 11:10:02|\n",
      "|5756852207|[20.5, BE, 0, 146...|         null|[16707, 1, [14, N...|[[12224991600, 9....|2019-12-13 11:10:02|\n",
      "|5756852206|[499.5, DE, 0, 14...|         null|[2852, 1, [14, No...|[[12224991598, 0....|2019-12-13 11:10:02|\n",
      "|5756852205|[54.2, BE, 0, 105...|         null|[20713, 1, [14, N...|[[12224991596, 0....|2019-12-13 11:10:02|\n",
      "|5756852204|[78.1, RO, 0, 168...|         null|[29961, 7, [9, va...|[[12224991594, 5....|2019-12-13 11:10:02|\n",
      "|5756852203|[54.8, DE, 0, 700...|         null|[13864, 1, [14, N...|[[12224991592, 0....|2019-12-13 11:10:02|\n",
      "|5756852202|[47.7, BE, 0, 838...|         null|[16552, 7, [9, va...|[[12224991590, 6....|2019-12-13 11:10:02|\n",
      "|5756852201|[40.5, BE, 1, 198...|         null|[33490, 7, [9, va...|[[12224991588, 7....|2019-12-13 11:10:02|\n",
      "|5756852200|[278.5, DE, 1, 16...|         null|[29226, 1, [14, N...|[[12224991586, 1....|2019-12-13 11:10:02|\n",
      "|5756852199|[184.9, DE, 0, 41...|         null|[8132, 7, [9, var...|[[12224991584, 4....|2019-12-13 11:10:02|\n",
      "|5756852198|[99.0, BE, 0, 195...|         null|[33171, 11, [17, ...|[[12224991581, 20...|2019-12-13 11:10:02|\n",
      "|5756852197|[339.6, DE, 0, 45...|         null|[8964, 1, [14, No...|[[12224991579, 38...|2019-12-13 11:10:02|\n",
      "|5756852196|[0.9, DE, 0, 1091...|         null|[21507, 1, [14, N...|[[12224991575, 26...|2019-12-13 11:10:02|\n",
      "|5756852195|[63.9, DE, 0, 254...|         null|[5042, 1, [14, No...|[[12224991577, 1....|2019-12-13 11:10:02|\n",
      "|5756852194|[59.8, DE, 0, 170...|         null|[3379, 1, [14, No...|[[12224991572, 1....|2019-12-13 11:10:02|\n",
      "|5756852193|[114.7, DE, 0, 11...|         null|[11872, 7, [9, va...|[[12224991571, 4....|2019-12-13 11:10:02|\n",
      "|5756852192|[284.0, DE, 0, 11...|         null|[2203, 1, [14, No...|[[12224991568, 1....|2019-12-13 11:10:02|\n",
      "|5756852191|[40.2, DE, 0, 811...|         null|[16015, 1, [14, N...|[[12224991566, 31...|2019-12-13 11:10:02|\n",
      "|5756852190|[29.1, IT, 0, 426...|         null|[8447, 7, [9, var...|[[12224991563, 14...|2019-12-13 11:10:02|\n",
      "+----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(functions.col(\"timestamp\").cast(\"date\").name(\"Date\") > \"2019-01-01\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除数据\n",
    "* 删除冗余数据 Spark 的 DataFrame 支持删除冗余数据，具有两种方法——[drop_duplicates()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop_duplicates) 和 [dropDuplicates()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropDuplicates)，此外同 pandas 一样支持 subset 参数调整删除依据。需要注意的是对于流数据删除操作和实际操作具有差异性以避免删除后再重复的可能性，详情可以参考文档\n",
    "* 删除缺失值 [pyspark.sql.DataFrame.drop_na()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropna) 和 [pyspark.sql.DataFrameNaFunctions.drop()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions.drop) 相同都是删除缺失值的方法，后者的使用方式是 `df.na.drop()`。同 pandas 一样支持 subset 参数调整。**需要注意，删除缺失值时，没有区分 NULL 以及 NAN 差别**\n",
    "* 删除列数据 [pyspark.sql.DataFrame.drop()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop) 支持直接删除列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除冗余行\n",
    "test.drop_duplicates(subset=[\"col1\", \"col5\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 21, 30)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面三者分别是是删除 col4 中的 NAN 以及 col1 中 NULL之后数量以及原始数量\n",
    "test.dropna(subset=[\"col4\"]).count(), test.dropna(subset=[\"col1\"]).count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35527"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据替换\n",
    "* 缺失值替换 [pyspark.sql.DataFrame.fillna()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.fillna) 和 [pyspark.sql.DataFrameNaFunctions.fill()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions.fill) 相同都是替换缺失值的方法，后者的使用方式是 `df.na.fill()`。 NAN 和 NULL 上，均会被替代\n",
    "* 指定数据替换 [pyspark.sql.DataFrame.replace()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.replace) 和 [pyspark.sql.DataFrameNaFunctions.replace()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions.replace)可以替换数据值。替换值的数据类型需要和原始数据类型相同\n",
    "* 条件性替换 使用方式类似于 `case when`。 [`functions.when()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.when) 和 [`column.when()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.when) 存在差异，后者和 [`column.otherwise()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.otherwise) 相似是对前者的扩展。实际功能性上和 SQL 中 `case...[when...then...]else...` 类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----+----+----+\n",
      "|col1|       col2|col3|col4|col5|\n",
      "+----+-----------+----+----+----+\n",
      "|   T|-10.2372465|  18| 3.0|   A|\n",
      "|   O|  1.5313411|   9| 0.0|   A|\n",
      "|   S| -9.1716175|   5| 0.0|   J|\n",
      "|   A|   7.406317|  19| 1.0|   A|\n",
      "|   X|   5.094148|   3| NaN|   X|\n",
      "|   E|  -3.525113|   3| 2.0|   A|\n",
      "|   W|  -18.93121|  10| 2.0|   R|\n",
      "|   A|  -4.990888|   3| NaN|   A|\n",
      "|   T| -7.7867312|   2| 3.0|   A|\n",
      "|   N| -14.736388|   6| 3.0|   A|\n",
      "|   A|    8.07055|  15| NaN|   A|\n",
      "|   A|  2.4262414|  19| 2.0|   K|\n",
      "|   A| -4.2632003|   2| NaN|   V|\n",
      "|   R| -3.1653242|   7| 0.0|   A|\n",
      "|   S|  27.000383|   8| 1.0|   A|\n",
      "|   A| -3.3306575|  10| 0.0|   E|\n",
      "|   Z|  12.456934|  19| NaN|   K|\n",
      "|   B| -3.7770996|  18| 2.0|   A|\n",
      "|   A|  -16.01117|   8| 3.0|   A|\n",
      "|   A| -1.0873488|   8| 3.0|   A|\n",
      "+----+-----------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 替换字符串\n",
    "test.fillna(\"A\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+----+----+----+\n",
      "|col1|       col2|col3|col4|col5|\n",
      "+----+-----------+----+----+----+\n",
      "|   T|-10.2372465|  18| 3.0|null|\n",
      "|   O|  1.5313411|   9| 0.0|null|\n",
      "|   S| -9.1716175|   5| 0.0|   J|\n",
      "|null|   7.406317|  19| 1.0|null|\n",
      "|   X|   5.094148|   3|12.0|   X|\n",
      "|   E|  -3.525113|   3| 2.0|null|\n",
      "|   W|  -18.93121|  10| 2.0|   R|\n",
      "|null|  -4.990888|   3|12.0|null|\n",
      "|   T| -7.7867312|   2| 3.0|null|\n",
      "|   N| -14.736388|   6| 3.0|null|\n",
      "|null|    8.07055|  15|12.0|null|\n",
      "|null|  2.4262414|  19| 2.0|   K|\n",
      "|null| -4.2632003|   2|12.0|   V|\n",
      "|   R| -3.1653242|   7| 0.0|null|\n",
      "|   S|  27.000383|   8| 1.0|null|\n",
      "|null| -3.3306575|  10| 0.0|   E|\n",
      "|   Z|  12.456934|  19|12.0|   K|\n",
      "|   B| -3.7770996|  18| 2.0|null|\n",
      "|null|  -16.01117|   8| 3.0|null|\n",
      "|null| -1.0873488|   8| 3.0|null|\n",
      "+----+-----------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.fillna(12).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他操作\n",
    "* 字符拼接 需要使用 `functions.concat()` 或者 `functions.concat_ws()` 可以完成相关任务，后者可以指定拼接间隔符\n",
    "* 日期计算 使用 `functions.date_add()`、`functions.date_sub()`——日期加减 `functions.datediff()`——日期间间隔天数，`functions.date_trunc()`——提取时间戳及日期中年月日以及时间点，可以试用格式 ‘year’, ‘yyyy’, ‘yy’, ‘month’, ‘mon’, ‘mm’, ‘day’, ‘dd’, ‘hour’, ‘minute’, ‘second’, ‘week’, ‘quarter’。其他类似于 Pandas 中 TimeStamp 的 Series 的属性提取操作，使用 `functions.dayofmonth()` 等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "1. [Unlike Pandas, PySpark doesn’t consider NaN values to be NULL. See the NaN Semantics](https://spark.apache.org/docs/latest/sql-reference.html#nan-semantics)\n",
    "2. [ Spark SQL, Built-in Functions](https://spark.apache.org/docs/latest/api/sql/)\n",
    "\n",
    "\n",
    "在调整数据过程中需要注意是否有必要进行原位替换，还是只需要部分数据——需要判断是使用 `withColumn()` 还是 `select()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": org.apache.spark.sql.catalyst.parser.ParseException: \n",
    "extraneous input ''' expecting {'(', 'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'ANY', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'PIVOT', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'FIRST', 'AFTER', 'LAST', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'DIRECTORY', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'COST', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IGNORE', 'BOTH', 'LEADING', 'TRAILING', 'IF', 'POSITION', 'EXTRACT', '+', '-', '*', 'DIV', '~', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', 'GLOBAL', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, DECIMAL_VALUE, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
