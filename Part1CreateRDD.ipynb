{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD 创建需要依赖于 SparkContext——它是用于连接 Spark 集群创建 RDD 以及广播集群中变量。\n",
    "\n",
    "1. [SparkContext 创建 Spark 应用](#SparkContext创建Spark应用)\n",
    "2. [SparkSession 创建 Spark 应用](#SparkSession创建Spark应用)\n",
    "3. [查看Spark应用配置信息](#查看Spark应用配置信息)\n",
    "\n",
    "\n",
    "### SparkContext创建Spark应用\n",
    "直接通过 SparkContext 创建，可以使用 SparkConf 对 Spark 应用进行设置， SparkConf 会默认加载 `spark.*` 的 Java 系统属性，也可以通过传入参数修改相关属性。进行单元测试时，可以使用 `loadDefaults=False` 参数来使用系统属性配置以保持测试一致。此外 SparkConf 还有其他方法用于设置，例如 `conf.setMaster(\"local\").setAppName(\"App\")`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过 SparkContext 创建\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# 通过 SparkConf 进行设相关属性，需要以 key-value 形式来进行设置\n",
    "config = SparkConf() \\\n",
    "        .set(\"Name\", \"ContextTest\") \\\n",
    "        .setMaster(\"local\") \\\n",
    "        .setAppName(\"First App\")\n",
    "\n",
    "\n",
    "# 创建 SparkContext\n",
    "sc = SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用parallelize创建RDD\n",
    "[parallelize](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.parallelize) 是用于创建 RDD 的便捷方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sc.parallelize(range(0, 10, 2), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 16, 36, 64]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.map(lambda a: a**2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'collect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ffc384e27995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'collect'"
     ]
    }
   ],
   "source": [
    "test.foreach(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession创建Spark应用\n",
    "SparkSession 是 应用中 DataFrame 和 SQL 的入口，可以用于创建 DataFrame、注册 DataFrame 为 Table（可以对 Table 使用 SQl 语句），也可以用于创建 RDD。SparkSession 创建也可以通过以下两种方式创建：\n",
    "1. 通过 builder 创建 对 Spark 应用的设置需要通过 SparkSession 下的相关方法和类（builder 可以用于配置后续相关设置，它是 SparkSession 类嵌套类的实例化）或者相关参数完成。注意在使用 conf 的配置方式和 SparkContext 的配置相似\n",
    "2. SparkSession 直接实例化 通过传入 SparkContext 作为参数进行实例化对象:\n",
    "    ```{python}\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    config = SparkConf() \\\n",
    "            .setMaster(\"local\") \\\n",
    "            .setAppName(\"First App\")\n",
    "\n",
    "    sc = SparkContext(conf=config)\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "    ss = SparkSession(sc)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparksession 和 sparkcontext 的导入模块不一致，这点需要注意\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"Name\", \"SessionTest\") \\\n",
    "    .appName(\"SessionApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看Spark应用配置信息\n",
    "获取 SparkContext 配置信息的方式，可以通过 getConf 方法以及其他 get 方法配合使用查看，例如：`sc.getConf().get(\"spark.master\")` 等，需要注意 Spark 应用的配置信息——基本上非配置的独特属性情况下，配置值都是以 `spark.*` 的形式生成的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.id', 'local-1575948343502'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.host', 'f6d772bd8388'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'First App'),\n",
       " ('spark.driver.port', '39263'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('Name', 'ContextTest')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 确认所有配置信息\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定配置信息\n",
    "sc.getConf().get(\"spark.master\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
